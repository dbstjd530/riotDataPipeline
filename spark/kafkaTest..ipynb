{
  "metadata": {
    "name": "kafkaTest",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Kafka Producer (API -\u003e Python -\u003e Kafka )\n\n## 01. 데이터 수집 "
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhadoop fs -rm -r -f /user/fastcampus/checkpoint\nhadoop fs -rm -r -f /user/fastcampus/sparkData\n\nhadoop fs -mkdir -p /user/fastcampus/checkpoint/structured_streaming\nhadoop fs -mkdir /user/fastcampus/sparkData\n\n\nhadoop fs -ls /user/fastcampus/sparkData\nhadoop fs -ls /user/fastcampus/checkpoint\n\nhadoop fs -mkdir /user/fastcampus/checkpoint/structured_streaming/lol_table_text\n\nrm -r /Users/yoonsung/fastcampus/zeppelin/zeppelin-0.10.1-bin-all/spark-warehouse/lol_table_text\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Spark Kafka 연동 (Kafka -\u003e Spark)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder.appName(\"MyApp\").getOrCreate()\nui_url \u003d spark.sparkContext.uiWebUrl\nprint(\"Spark UI URL:\", ui_url)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"KafkaConsumer\") \\\n    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\u0027ERROR\u0027)\n\n\nlol \u003d spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"ypgg\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n    \nlol.printSchema()\nprint(lol.isStreaming)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlol_value \u003d lol.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n                .select(\"value\")\n\n        \nlol_value.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf_stream_lol \u003d lol_value \\\n.filter(\"value is not null\") \\\n.filter(\"value \u003c\u003e \u0027\u0027\") \\\n.toDF(\"history\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.streaming import DataStreamWriter\n\n# Write stream - HDFS\nquery_df_stream_lol_hdfs_text \u003d df_stream_lol.selectExpr(\"CAST(history AS STRING)\") \\\n.writeStream \\\n.trigger(processingTime \u003d \u00275 seconds\u0027) \\\n.outputMode(\"append\") \\\n.format(\"text\") \\\n.option(\"checkpointLocation\", \"/user/fastcampus/checkpoint/structured_streaming/lol_hdfs_text\") \\\n.option(\"path\",\"/user/fastcampus/sparkData\") \\\n.option(\"encoding\", \"utf-8\") \\\n.queryName(\"query_df_stream_lol_hdfs_text\") \\\n.start()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(query_df_stream_lol_hdfs_text.status)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nquery_df_stream_lol_hdfs_text.stop()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nhadoop fs -ls /user/fastcampus/sparkData\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhadoop fs -cat /user/fastcampus/sparkData/part-00000-e5696a29-e015-43ac-a756-6347fe1d95fa-c000.txt"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.streaming import DataStreamWriter\n\n\nquery_df_stream_lol_table_text \u003d df_stream_lol \\\n    .writeStream \\\n    .trigger(processingTime \u003d \u00275 seconds\u0027) \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/user/fastcampus/checkpoint/structured_streaming/lol_table_text\") \\\n    .queryName(\"query_df_stream_lol_table_text\") \\\n    .toTable(\"lol_table_text\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nquery_df_stream_lol_table_text.stop()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nprint(spark.catalog.listTables())\n\nspark.table(\"lol_table_text\").printSchema()\n\nspark.catalog.refreshTable(\"lol_table_text\")\n\nprint(spark.table(\"lol_table_text\").count())"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nquery_df_stream_lol_memory_text \u003d df_stream_lol \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"memory\") \\\n    .queryName(\"query_df_stream_lol_memory_text\") \\\n    .start()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(spark.catalog.listTables())\n\nspark.table(\"query_df_stream_lol_memory_text\").printSchema()\n\n\nprint(spark.table(\"query_df_stream_lol_memory_text\").count())"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(query_df_stream_lol_memory_text.status)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.catalog.refreshTable(\"lol_table_text\")\n\ndf_all_lol_warehouse \u003d spark.read.json(spark.table(\"lol_table_text\").select(\"history\").rdd.flatMap(lambda x: x))\n\nz.show(df_all_lol_warehouse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n\ndf_all_lol_warehouse \u003d df_all_lol_warehouse \\\n  .withColumn(\"champion_name\", col(\"championInfo\").getItem(\"championName\")) \\\n  .withColumn(\"champion_level\", col(\"championInfo\").getItem(\"championLevel\")) \\\n  .withColumn(\"kill_streak\", col(\"championInfo\").getItem(\"killStreak\")) \\\n  .withColumn(\"kill\", col(\"championInfo\").getItem(\"kill\")) \\\n  .withColumn(\"death\", col(\"championInfo\").getItem(\"death\")) \\\n  .withColumn(\"assist\", col(\"championInfo\").getItem(\"assist\")) \\\n  .withColumn(\"lane\", col(\"championInfo\").getItem(\"lane\")) \\\n  .withColumn(\"vision_ward_count\", col(\"championInfo\").getItem(\"visionWardCount\")) \\\n  .withColumn(\"minion_count\", col(\"championInfo\").getItem(\"minionCOunt\")) \\\n  .withColumn(\"spell_1\", col(\"championInfo\").getItem(\"spells\").getItem(0).getItem(0)) \\\n  .withColumn(\"spell_2\", col(\"championInfo\").getItem(\"spells\").getItem(1).getItem(0)) \\\n  .withColumn(\"items_1\", col(\"championInfo\").getItem(\"items\").getItem(0)) \\\n  .withColumn(\"items_2\", col(\"championInfo\").getItem(\"items\").getItem(1)) \\\n  .withColumn(\"items_3\", col(\"championInfo\").getItem(\"items\").getItem(2)) \\\n  .withColumn(\"items_4\", col(\"championInfo\").getItem(\"items\").getItem(3)) \\\n  .withColumn(\"items_5\", col(\"championInfo\").getItem(\"items\").getItem(4)) \\\n  .withColumn(\"items_6\", col(\"championInfo\").getItem(\"items\").getItem(5)) \\\n  .withColumn(\"items_7\", col(\"championInfo\").getItem(\"items\").getItem(6)) \\\n  .withColumn(\"primary_rune\", col(\"championInfo\").getItem(\"runes\").getItem(\"primaryRunes\").getItem(1)) \\\n  .withColumn(\"sub_rune\", col(\"championInfo\").getItem(\"runes\").getItem(\"subRunes\").getItem(0))\n\ndf_all_lol_warehouse \u003d df_all_lol_warehouse.drop(\"championInfo\")\n\nz.show(df_all_lol_warehouse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_all_lol_warehouse.columns\n\ndf_all_lol_warehouse \u003d df_all_lol_warehouse.select(df_all_lol_warehouse.fewHoursGame, df_all_lol_warehouse.matchId, df_all_lol_warehouse.gameMode, df_all_lol_warehouse.tier, df_all_lol_warehouse.gameDuration, df_all_lol_warehouse.champion_name, \\\n                                                    df_all_lol_warehouse.lane, df_all_lol_warehouse.champion_level, df_all_lol_warehouse.kill, df_all_lol_warehouse.death, df_all_lol_warehouse.assist, df_all_lol_warehouse.kill_streak, \\\n                                                    df_all_lol_warehouse.spell_1, df_all_lol_warehouse.spell_2, df_all_lol_warehouse.primary_rune, df_all_lol_warehouse.sub_rune, \\\n                                                    df_all_lol_warehouse.items_1, df_all_lol_warehouse.items_2, df_all_lol_warehouse.items_3, df_all_lol_warehouse.items_4, df_all_lol_warehouse.items_5, \\\n                                                    df_all_lol_warehouse.items_6, df_all_lol_warehouse.items_7, df_all_lol_warehouse.minion_count, df_all_lol_warehouse.vision_ward_count, df_all_lol_warehouse.outCome)\n                     \n\nz.show(df_all_lol_warehouse)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf_all_lol_warehouse.filter(df_all_lol_warehouse.gameMode \u003d\u003d \"CLASSIC\") \\\n                    .dropDuplicates() \\\n                    .createOrReplaceTempView(\"lol_agg_table\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nlane_count \u003d spark.sql(\"SELECT lane, count(lane) AS lane_count FROM lol_agg_table GROUP BY lane ORDER BY lane_count desc\")\n\nz.show(lane_count)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n \ntop \u003d spark.sql(\"SELECT champion_name AS top_champion_name, count(champion_name) AS total_champions FROM lol_agg_table WHERE lane \u003d \u0027TOP\u0027 GROUP BY champion_name ORDER BY total_champions desc LIMIT 10\")\njungle \u003d spark.sql(\"SELECT champion_name AS jungle_champion_name, count(champion_name) AS total_champions FROM lol_agg_table WHERE lane \u003d \u0027JUNGLE\u0027 GROUP BY champion_name ORDER BY total_champions desc LIMIT 10\")\nmiddle \u003d spark.sql(\"SELECT champion_name AS middle_champion_name, count(champion_name) AS total_champions FROM lol_agg_table WHERE lane \u003d \u0027MIDDLE\u0027 GROUP BY champion_name ORDER BY total_champions desc LIMIT 10\")\nbottom \u003d spark.sql(\"SELECT champion_name AS bottom_champion_name, count(champion_name) AS total_champions FROM lol_agg_table WHERE lane \u003d \u0027BOTTOM\u0027 GROUP BY champion_name ORDER BY total_champions desc LIMIT 10\")\nsupport \u003d spark.sql(\"SELECT champion_name AS support_champion_name, count(champion_name) AS total_champions FROM lol_agg_table WHERE lane \u003d \u0027UTILITY\u0027 GROUP BY champion_name ORDER BY total_champions desc LIMIT 10\")\n\n\nz.show(top)\nz.show(jungle)\nz.show(middle)\nz.show(bottom)\nz.show(support)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import round\n\ntop \u003d spark.sql(\"\"\"\n    SELECT l.champion_name,  l.total_count AS total_count, v.victory_count AS victory_count, CAST(ROUND((v.victory_count / l.total_count) * 100) AS INTEGER) AS victory_rate\n    FROM (\n        SELECT champion_name, COUNT(champion_name) AS total_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027TOP\u0027\n        GROUP BY champion_name\n    ) l\n    JOIN (\n        SELECT champion_name, COUNT(champion_name) AS victory_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027TOP\u0027 AND outCome \u003d \u0027Victory\u0027\n        GROUP BY champion_name\n    ) v\n    ON l.champion_name \u003d v.champion_name\n    ORDER BY total_count desc\n    LIMIT 10\n\"\"\")\n\njungle \u003d spark.sql(\"\"\"\n    SELECT l.champion_name,  l.total_count AS total_count, v.victory_count AS victory_count, CAST(ROUND((v.victory_count / l.total_count) * 100) AS INTEGER) AS victory_rate\n    FROM (\n        SELECT champion_name, COUNT(champion_name) AS total_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027JUNGLE\u0027\n        GROUP BY champion_name\n    ) l\n    JOIN (\n        SELECT champion_name, COUNT(champion_name) AS victory_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027JUNGLE\u0027 AND outCome \u003d \u0027Victory\u0027\n        GROUP BY champion_name\n    ) v\n    ON l.champion_name \u003d v.champion_name\n    ORDER BY total_count desc\n    LIMIT 10\n\"\"\")\n\nmiddle \u003d spark.sql(\"\"\"\n    SELECT l.champion_name,  l.total_count AS total_count, v.victory_count AS victory_count, CAST(ROUND((v.victory_count / l.total_count) * 100) AS INTEGER) AS victory_rate\n    FROM (\n        SELECT champion_name, COUNT(champion_name) AS total_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027MIDDLE\u0027\n        GROUP BY champion_name\n    ) l\n    JOIN (\n        SELECT champion_name, COUNT(champion_name) AS victory_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027MIDDLE\u0027 AND outCome \u003d \u0027Victory\u0027\n        GROUP BY champion_name\n    ) v\n    ON l.champion_name \u003d v.champion_name\n    ORDER BY total_count desc\n    LIMIT 10\n\"\"\")\n\nbottom \u003d spark.sql(\"\"\"\n    SELECT l.champion_name,  l.total_count AS total_count, v.victory_count AS victory_count, CAST(ROUND((v.victory_count / l.total_count) * 100) AS INTEGER) AS victory_rate\n    FROM (\n        SELECT champion_name, COUNT(champion_name) AS total_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027BOTTOM\u0027\n        GROUP BY champion_name\n    ) l\n    JOIN (\n        SELECT champion_name, COUNT(champion_name) AS victory_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027BOTTOM\u0027 AND outCome \u003d \u0027Victory\u0027\n        GROUP BY champion_name\n    ) v\n    ON l.champion_name \u003d v.champion_name\n    ORDER BY total_count desc\n    LIMIT 10\n\"\"\")\n\nsupport \u003d spark.sql(\"\"\"\n    SELECT l.champion_name,  l.total_count AS total_count, v.victory_count AS victory_count, CAST(ROUND((v.victory_count / l.total_count) * 100) AS INTEGER) AS victory_rate\n    FROM (\n        SELECT champion_name, COUNT(champion_name) AS total_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027UTILITY\u0027\n        GROUP BY champion_name\n    ) l\n    JOIN (\n        SELECT champion_name, COUNT(champion_name) AS victory_count\n        FROM lol_agg_table\n        WHERE lane \u003d \u0027UTILITY\u0027 AND outCome \u003d \u0027Victory\u0027\n        GROUP BY champion_name\n    ) v\n    ON l.champion_name \u003d v.champion_name\n    ORDER BY total_count desc\n    LIMIT 10\n\"\"\")\n\n\n\nz.show(top)\nz.show(jungle)\nz.show(middle)\nz.show(bottom)\nz.show(support)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import functions as F\n\n# items_1에 대한 count\nitems_1 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_1 AS item, count(items_1) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_1\n\"\"\")\n\n# items_2에 대한 count\nitems_2 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_2 AS item, count(items_2) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_2\n\"\"\")\n\n# items_3에 대한 count\nitems_3 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_3 AS item, count(items_3) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_3\n\"\"\")\n\n# items_4에 대한 count\nitems_4 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_4 AS item, count(items_4) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_4\n\"\"\")\n\n# items_5에 대한 count\nitems_5 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_5 AS item, count(items_5) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_5\n\"\"\")\n\n# items_6에 대한 count\nitems_6 \u003d spark.sql(\"\"\"\n    SELECT champion_name, items_6 AS item, count(items_6) AS count\n    FROM lol_agg_table\n    WHERE lane \u003d \u0027TOP\u0027 and outCome \u003d \u0027Victory\u0027\n    GROUP BY champion_name, items_6\n\"\"\")\n\n# 조인 및 합산\njoined_items \u003d items_1.join(items_2, [\u0027champion_name\u0027, \u0027item\u0027], \u0027inner\u0027) \\\n    .join(items_3, [\u0027champion_name\u0027, \u0027item\u0027], \u0027inner\u0027) \\\n    .join(items_4, [\u0027champion_name\u0027, \u0027item\u0027], \u0027inner\u0027) \\\n    .join(items_5, [\u0027champion_name\u0027, \u0027item\u0027], \u0027inner\u0027) \\\n    .join(items_6, [\u0027champion_name\u0027, \u0027item\u0027], \u0027inner\u0027) \\\n    .select(items_1[\u0027champion_name\u0027], items_1[\u0027item\u0027], (items_1[\u0027count\u0027] + items_2[\u0027count\u0027] + items_3[\u0027count\u0027] + items_4[\u0027count\u0027] + items_5[\u0027count\u0027] + items_6[\u0027count\u0027]).alias(\u0027total_count\u0027)) \\\n    .orderBy(F.desc(\u0027champion_name\u0027))\n\n# 결과 출력\nz.show(joined_items)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}